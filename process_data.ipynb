{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0782976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60dcc8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version:  1.19.2\n",
      "Scipy version:  1.6.2\n"
     ]
    }
   ],
   "source": [
    "# print numpy version\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "# print scipy version\n",
    "print(\"Scipy version: \", scipy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26d0a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PyTorch Version: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded PyTorch Version: {}\".format(torch.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8a5a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184bff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.8.20 (default, Oct  3 2024, 15:24:27) \n",
      "[GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "# find the python version\n",
    "import sys\n",
    "print(\"Python version: \", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224fa85f",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26115337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class DiscardDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    class DiscardType(Enum):\n",
    "        DISCARD = 0\n",
    "        POOL    = 1\n",
    "\n",
    "    def __init__(self, data_path, years: list, n_rows: int = None, phase: int = None, balance_data: bool = False, discard_type=DiscardType.DISCARD, singular=False):\n",
    "        \"\"\" \n",
    "        If n_rows = None -> get all \n",
    "        param: singular: If True, pick 1 state per game at random!\n",
    "        \"\"\" \n",
    "        \n",
    "        # FORCE SEED\n",
    "        torch.manual_seed(0)\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        # Invalid Parameter Combinations\n",
    "        if balance_data:\n",
    "            if not n_rows:\n",
    "                raise BaseException(\"`n_rows` must be defined if `balance_data` is True!\")\n",
    "            elif n_rows < 34:\n",
    "                raise BaseException(\"Cannot balance data if `n_rows` < 34!\")\n",
    "        \n",
    "        ALL_YEARS = (2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019)\n",
    "        invalid_years = set(years) - set(ALL_YEARS)\n",
    "        if invalid_years:\n",
    "            raise Exception(f\"INVALID YEARS: {invalid_years}\")\n",
    "        \n",
    "        # Dataset Print\n",
    "        if n_rows:\n",
    "            print(f\"Loading Dataset with {n_rows:>13,} rows\", end=' ')\n",
    "        else:\n",
    "            print(f\"Loading Dataset with all rows\", end=' ')\n",
    "\n",
    "        if phase in [0, 1, 2]:\n",
    "            print(f\"(Phase {phase})\", end=' ')\n",
    "        else:\n",
    "            print(f\"(All Phases)\", end=' ')\n",
    "            \n",
    "        print(\"{:<14}\".format(\"<BALANCED>\" if balance_data else '<NOT BALANCED>'), end=' ')\n",
    "        \n",
    "        print(years)\n",
    "        \n",
    "        # Check if given discard_type is valid\n",
    "        if discard_type not in [DiscardDataset.DiscardType.DISCARD, DiscardDataset.DiscardType.POOL]:\n",
    "            raise BaseException(f\"INVALID discard type = {discard_type}! Use either `DiscardDataset.DISCARD` or `DiscardDataset.POOL`!\")\n",
    "        self.discard_type = discard_type\n",
    "\n",
    "        game_id_list = []\n",
    "        temp_matrices = []\n",
    "        finished = False\n",
    "        \n",
    "        # Used when balance_data = False and n_rows != None\n",
    "        loaded_rows = 0  \n",
    "        \n",
    "        # Used when balance_data = True\n",
    "        class_bins = np.zeros(34)\n",
    "        baseline_bin_size = n_rows // 34 if balance_data else -1  # The expected size of the smallest bin\n",
    "\n",
    "        if balance_data:\n",
    "            paths_load_bar = tqdm(total=baseline_bin_size * 34, unit='rows', position=0)\n",
    "        else:\n",
    "            paths_load_bar = tqdm(total=n_rows, unit='rows', position=0)\n",
    "\n",
    "        for year in years:\n",
    "\n",
    "            paths = (Path(data_path) / str(year)).iterdir()\n",
    "\n",
    "            for idx, path in enumerate(paths):\n",
    "                \n",
    "                if path.suffix != '.npz':\n",
    "                    continue\n",
    "                \n",
    "                game_id_list.append(path.stem)\n",
    "\n",
    "                arr = scipy.sparse.load_npz(path).toarray()  # Loads a single complete game\n",
    "\n",
    "                if phase in [0, 1, 2]:\n",
    "                    phased_matrices = self.generate_phase_column(arr)\n",
    "                    arr = phased_matrices[phase]\n",
    "                    \n",
    "                if singular:\n",
    "                    if arr.shape[0] <= 0:  # No rows found (This can happen if a game lack states from a certain phase)\n",
    "                        continue\n",
    "                    random_row_index = np.random.choice(arr.shape[0], 1, replace=False)\n",
    "                    arr = arr[random_row_index]  # Select 1 row per loaded game\n",
    "\n",
    "                temp_matrices.append(arr)\n",
    "\n",
    "                paths_load_bar.set_postfix(year=year, files_loaded=(idx + 1))  # Update Bar\n",
    "\n",
    "                if balance_data:\n",
    "                    \n",
    "                    class_bins += np.bincount(arr[:, -1], minlength=34)\n",
    "                    smallest_class_bin = int(np.amin(class_bins))\n",
    "\n",
    "                    paths_load_bar.n = smallest_class_bin * 34\n",
    "                    paths_load_bar.refresh()\n",
    "                    \n",
    "                    if baseline_bin_size <= smallest_class_bin:\n",
    "                        finished = True\n",
    "                        break\n",
    "\n",
    "                else:\n",
    "                    paths_load_bar.update(arr.shape[0])\n",
    "                    \n",
    "                    if n_rows:\n",
    "                        loaded_rows += arr.shape[0]\n",
    "                        if n_rows <= loaded_rows:\n",
    "                            finished = True\n",
    "                            break\n",
    "\n",
    "            if finished:  # Early Stopping\n",
    "                break\n",
    "\n",
    "        if not finished and n_rows is not None:\n",
    "            raise BaseException(\"`n_rows` is higher than found rows -- Either lower `n_rows` or include more annual datasets!\")\n",
    "\n",
    "        new_game_id_list = []\n",
    "        for i, t_matrix in enumerate(temp_matrices):\n",
    "            new_game_id_list.extend([game_id_list[i]] * t_matrix.shape[0])\n",
    "        new_game_id_list = np.array(new_game_id_list)\n",
    "        \n",
    "        if balance_data:\n",
    "            \n",
    "            matrix = np.concatenate(temp_matrices, axis=0)\n",
    "            sorted_indices = np.argsort(matrix[:, -1])\n",
    "            \n",
    "            matrix = matrix[sorted_indices]  # Sort rows by last column (the y-value)\n",
    "            new_game_id_list = new_game_id_list[sorted_indices]\n",
    "            \n",
    "            split_indices = np.where(np.diff(matrix[:, -1])!=0)[0]+1  # I was drunk\n",
    "            sorted_rows = np.array_split(matrix, split_indices)  # Organize rows according to their last column's value into a list\n",
    "            sorted_game_ids = np.array_split(new_game_id_list, split_indices)\n",
    "            \n",
    "            for i in range(len(sorted_rows)):\n",
    "                sorted_rows[i] = sorted_rows[i][:baseline_bin_size]  # The balancing action\n",
    "                sorted_game_ids[i] = sorted_game_ids[i][:baseline_bin_size]\n",
    "\n",
    "            final_arr = np.concatenate(sorted_rows, axis=0)\n",
    "            final_game_id_list = np.concatenate(sorted_game_ids, axis=0)\n",
    "\n",
    "        else:\n",
    "            final_arr = np.vstack(temp_matrices)\n",
    "            final_game_id_list = new_game_id_list\n",
    "            \n",
    "            if n_rows:\n",
    "                final_arr = final_arr[:n_rows]\n",
    "                final_game_id_list = final_game_id_list[:n_rows]\n",
    "                \n",
    "        # Extract Round Number and Steps from data\n",
    "        self.round_numbers = final_arr[:, 32].reshape(-1).tolist()\n",
    "        self.step_numbers  = (final_arr[:, 33] + 128 - 1).reshape(-1).tolist()        \n",
    "        final_arr[:, 32] = -128  # Reset to padding value\n",
    "        final_arr[:, 33] = -128  # Reset to padding value\n",
    "\n",
    "        # Finalize tqdm bar\n",
    "        paths_load_bar.n = final_arr.shape[0]\n",
    "        paths_load_bar.last_print_n = final_arr.shape[0]\n",
    "        paths_load_bar.refresh()\n",
    "        paths_load_bar.close()\n",
    "        \n",
    "        self.game_ids = list(final_game_id_list)\n",
    "        self.combined_x_data = torch.FloatTensor(final_arr[:, :-1])  # Must be Float it seems\n",
    "        \n",
    "        self.x_data = None\n",
    "        if self.discard_type == DiscardDataset.DiscardType.POOL:\n",
    "            self.use_pools()\n",
    "        else:\n",
    "            self.use_discards()\n",
    "        \n",
    "        self.y_data = torch.LongTensor(final_arr[:, -1])  # Must be Long it seems\n",
    "        \n",
    "    def use_pools(self):\n",
    "        self.discard_type = DiscardDataset.DiscardType.POOL\n",
    "        self.x_data = self.combined_x_data[:, 0:374]\n",
    "\n",
    "    def use_discards(self):\n",
    "        self.discard_type = DiscardDataset.DiscardType.DISCARD\n",
    "        self.x_data = torch.hstack((self.combined_x_data[:, :238], self.combined_x_data[:, 374:]))  # Slice away POOL data\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_phase_column(array: np.array) -> np.array:\n",
    "        # Begin with merging all pools together\n",
    "\n",
    "        merged_discards = array[:, 238:374]  # Pool\n",
    "        merged_discards = np.sum(merged_discards, axis=1)\n",
    "\n",
    "        phases = np.zeros([array.shape[0]])  # Early Game\n",
    "        phases[(24 < merged_discards) & (merged_discards <= 48)] = 1  # Mid Game\n",
    "        phases[(48 < merged_discards)] = 2  # End Game\n",
    "\n",
    "        return array[(phases == 0)], array[(phases == 1)], array[(phases == 2)]        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         X = self.x_data[idx, 0:374] if self.discard_type == DiscardDataset.DiscardType.POOL else torch.hstack((self.x_data[idx, :238], self.x_data[idx, 374:]))\n",
    "        return {\n",
    "            'game_id': self.game_ids[idx],\n",
    "            'round': self.round_numbers[idx],\n",
    "            'step': self.step_numbers[idx],\n",
    "            'X': self.x_data[idx],\n",
    "            'y': self.y_data[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beebdefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(469, 511)\n"
     ]
    }
   ],
   "source": [
    "file = '/home/lenzlaww/document/SBU/CSE537/finalProject/mjx/2019/2019010100gm-00a9-0000-127203b5.npz'\n",
    "npz = np.load(file)\n",
    "mat1 = np.array(csr_matrix((npz['data'], npz['indices'], npz['indptr']), shape=npz['shape']).toarray())\n",
    "print(mat1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17afe6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOADING DATASETS:\n",
      "\n",
      "Total .npz files: 171629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('2019/2019012700gm-00a9-0000-c920cee3.npz'),\n",
       " PosixPath('2019/2019071617gm-00a9-0000-0baa04f0.npz'),\n",
       " PosixPath('2019/2019100811gm-00a9-0000-6fc9b38a.npz'),\n",
       " PosixPath('2019/2019060921gm-00a9-0000-1786343a.npz'),\n",
       " PosixPath('2019/2019110621gm-00a9-0000-636733c5.npz')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nLOADING DATASETS:\\n\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "dataset_path = Path(\"2019\")\n",
    "npz_files = [f for f in dataset_path.iterdir() if f.suffix == '.npz']\n",
    "print(f\"Total .npz files: {len(npz_files)}\")\n",
    "npz_files[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719df9be",
   "metadata": {},
   "source": [
    "# Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\" Simple Feed-Forward Net \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.name = \"BasicDenseNetwork_testing\"\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(11 * 34, 1024)  # SWITCH TO 1024\n",
    "        self.fc2 = torch.nn.Linear(1024, 512)      # SWITCH TO 1024\n",
    "        self.fc3 = torch.nn.Linear(512, 256)\n",
    "        self.fc4 = torch.nn.Linear(256, 128)\n",
    "        self.fc5 = torch.nn.Linear(128, 34)\n",
    "        \n",
    "        self.relu_1 = torch.nn.LeakyReLU()\n",
    "        self.relu_2 = torch.nn.LeakyReLU()\n",
    "        self.relu_3 = torch.nn.LeakyReLU()\n",
    "        self.relu_4 = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "            \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu_1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu_2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu_3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.relu_4(x)\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
