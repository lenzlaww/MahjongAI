{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd18ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import mjx\n",
    "from mjx.agents import RandomAgent, ShantenAgent\n",
    "from ppo_agent import PPOAgent, GymEnv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment: Play against Shanten agent\n",
    "opponents = [ShantenAgent() for _ in range(3)]  \n",
    "env = GymEnv(opponent_agents=opponents)\n",
    "\n",
    "# obtain the observation and action mask shape\n",
    "obs, info = env.reset()\n",
    "obs_shape = obs.flatten().shape[0]\n",
    "action_dim = len(info[\"action_mask\"])  # action number\n",
    "\n",
    "# Initialize PPO agent\n",
    "agent = PPOAgent(\n",
    "    input_dim=obs_shape,\n",
    "    hidden_dim=128,\n",
    "    output_dim=action_dim,\n",
    "    lr = 1e-4,              # small learning rate\n",
    "    entropy_coef=0.001      # small entropy coefficient: small curiousity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef02922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards, path=\"logs/reward_curve.png\"):\n",
    "    plt.figure()\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Training Reward Curve\")\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ccfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/3000, avg reward: -125.550\n",
      "Best model saved with reward: -125.550\n",
      "Episode 200/3000, avg reward: -130.950\n",
      "Episode 300/3000, avg reward: -126.900\n",
      "Episode 400/3000, avg reward: -123.750\n",
      "Best model saved with reward: -123.750\n",
      "Episode 500/3000, avg reward: -122.850\n",
      "Best model saved with reward: -122.850\n",
      "Episode 600/3000, avg reward: -128.250\n",
      "Episode 700/3000, avg reward: -123.300\n",
      "Episode 800/3000, avg reward: -128.250\n",
      "Episode 900/3000, avg reward: -122.850\n",
      "Episode 1000/3000, avg reward: -121.500\n",
      "Best model saved with reward: -121.500\n",
      "Episode 1100/3000, avg reward: -123.300\n",
      "Episode 1200/3000, avg reward: -130.950\n",
      "Episode 1300/3000, avg reward: -122.850\n",
      "Episode 1400/3000, avg reward: -132.300\n",
      "Episode 1500/3000, avg reward: -125.550\n",
      "Episode 1600/3000, avg reward: -128.250\n",
      "Episode 1700/3000, avg reward: -126.900\n",
      "Episode 1800/3000, avg reward: -128.250\n",
      "Episode 1900/3000, avg reward: -130.950\n",
      "Episode 2000/3000, avg reward: -131.850\n",
      "Episode 2100/3000, avg reward: -123.300\n",
      "Episode 2200/3000, avg reward: -126.900\n",
      "Episode 2300/3000, avg reward: -121.050\n",
      "Best model saved with reward: -121.050\n",
      "Episode 2400/3000, avg reward: -118.800\n",
      "Best model saved with reward: -118.800\n",
      "Episode 2500/3000, avg reward: -125.550\n",
      "Episode 2600/3000, avg reward: -114.750\n",
      "Best model saved with reward: -114.750\n",
      "Episode 2700/3000, avg reward: -124.200\n",
      "Episode 2800/3000, avg reward: -122.400\n",
      "Episode 2900/3000, avg reward: -126.900\n",
      "Episode 3000/3000, avg reward: -128.250\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# training parameters\n",
    "num_episodes = 3000\n",
    "log_interval = 100  # Log every 100 episodes\n",
    "rolling_rewards = []\n",
    "\n",
    "all_rewards = []\n",
    "all_actor_loss = []\n",
    "all_value_loss = []\n",
    "best_reward = -float(\"inf\")\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(obs, info[\"action_mask\"])\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        agent.store_reward(reward)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    \n",
    "    stats = agent.update()\n",
    "    all_rewards.append(total_reward)\n",
    "    all_actor_loss.append(stats['actor_loss'])\n",
    "    all_value_loss.append(stats['value_loss'])\n",
    "\n",
    "    # print statistics\n",
    "    if episode % log_interval == 0:\n",
    "        avg_reward = np.mean(all_rewards[-log_interval:])\n",
    "        rolling_rewards.append(avg_reward)\n",
    "        print(f\"Episode {episode}/{num_episodes}, avg reward: {avg_reward:.3f}\")\n",
    "        # Update the best model if the average reward is higher\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            torch.save(agent.model.state_dict(), \"logs/ppo_shanten_opponent_model.pt\")\n",
    "            print(f\"Best model saved with reward: {best_reward:.3f}\")\n",
    "\n",
    "        log_data = {\n",
    "            'episode': episode,\n",
    "            'avg_reward': avg_reward,\n",
    "            'actor_loss': stats['actor_loss'],\n",
    "            'value_loss': stats['value_loss'],\n",
    "            'entropy': stats['entropy'],\n",
    "            'total_loss': stats['total_loss']\n",
    "        }\n",
    "        with open('logs/ppo_shanten_opponent_training_log.json', 'a') as f:\n",
    "            json.dump(log_data, f)\n",
    "            f.write('\\n')\n",
    "# plot the reward curve\n",
    "plot_rewards(rolling_rewards, path=f\"logs/ppo_shanten_opponent_reward_curve_{episode}.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaafbba4",
   "metadata": {},
   "source": [
    "训练时长：202m34.5s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
