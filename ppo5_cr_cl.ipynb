{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d674713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import mjx\n",
    "from mjx.agents import RandomAgent, ShantenAgent  # 使用 Shanten agent\n",
    "from utils import compute_reward\n",
    "from collections import OrderedDict\n",
    "\n",
    "class GymEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self, \n",
    "        opponent_agents: list, \n",
    "        reward_type: str = \"game_tenhou_7dan\", \n",
    "        done_type: str = \"game\", \n",
    "        feature_type: str = \"mjx-small-v0\",\n",
    "        info_type: str = \"perfect\"\n",
    "    ):\n",
    "        super(GymEnv, self).__init__()\n",
    "        self.opponent_agents = opponent_agents  # 对手列表，包含 Shanten agent\n",
    "        self.reward_type = reward_type\n",
    "        self.done_type = done_type\n",
    "        self.feature_type = feature_type\n",
    "\n",
    "        self.target_player = \"player_0\"  # 我方玩家\n",
    "        self.mjx_env = mjx.MjxEnv()\n",
    "        self.curr_obs_dict = self.mjx_env.reset()\n",
    "\n",
    "        self.prev_obs = None\n",
    "        self.info_type = info_type\n",
    "        \n",
    "        obs = next(iter(self.curr_obs_dict.values()))\n",
    "        sample_feat = obs.to_features(self.feature_type)\n",
    "\n",
    "        self.full_info = OrderedDict(\n",
    "            (f\"player_{i}\", np.zeros_like(sample_feat)) for i in range(4)\n",
    "        )\n",
    "\n",
    "    def _update_full_info(self, obs_dict):\n",
    "        # print(obs_dict)\n",
    "        for player_id, obs in obs_dict.items():\n",
    "            self.full_info[player_id] = obs.to_features(self.feature_type)\n",
    "    \n",
    "    def _set_opponent_agents(self, agents):\n",
    "        \"\"\"设置对手代理\"\"\"\n",
    "        self.opponent_agents = agents\n",
    "\n",
    "    def _set_info_type(self, info_type):\n",
    "        \"\"\"设置信息类型\"\"\"\n",
    "        self.info_type = info_type\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"环境重置，返回第一个状态和可用动作\"\"\"\n",
    "        self.curr_obs_dict = self.mjx_env.reset()\n",
    "        obs = next(iter(self.curr_obs_dict.values()))\n",
    "        sample_feat = obs.to_features(self.feature_type)\n",
    "        self.full_info = {\n",
    "            f\"player_{i}\": np.zeros_like(sample_feat) for i in range(4)\n",
    "        }\n",
    "        self.prev_obs = None\n",
    "\n",
    "        # 跳过其他玩家的回合，直到轮到我们的玩家\n",
    "        while self.target_player not in self.curr_obs_dict:\n",
    "            action_dict = {\n",
    "                player_id: self.opponent_agents[i].act(obs)\n",
    "                for i, (player_id, obs) in enumerate(self.curr_obs_dict.items())\n",
    "            }\n",
    "            self.curr_obs_dict = self.mjx_env.step(action_dict)\n",
    "        self._update_full_info(self.curr_obs_dict)\n",
    "\n",
    "        # 返回当前玩家的特征\n",
    "        obs = self.curr_obs_dict[self.target_player]\n",
    "        if self.info_type == \"perfect\":\n",
    "            feat = np.concatenate(list(self.full_info.values()), axis=-1)\n",
    "        else:\n",
    "            feat = obs.to_features(self.feature_type)\n",
    "        mask = obs.action_mask()  # 获取有效动作\n",
    "        return feat, {\"action_mask\": mask}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"执行一个步骤，并返回下一个状态、奖励、是否结束\"\"\"\n",
    "        action_dict = {self.target_player: mjx.Action.select_from(action, self.curr_obs_dict[self.target_player].legal_actions())}\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # 遍历当前所有玩家\n",
    "        for i, (pid, obs) in enumerate(self.curr_obs_dict.items()):\n",
    "            if pid != self.target_player:\n",
    "                # 需要将 i-1 映射到对手代理列表\n",
    "                action_dict[pid] = self.opponent_agents[i-1].act(obs)\n",
    "\n",
    "        # 更新状态\n",
    "        self.curr_obs_dict = self.mjx_env.step(action_dict)\n",
    "        self._update_full_info(self.curr_obs_dict)\n",
    "        \n",
    "\n",
    "        # 检查是否是当前玩家的回合\n",
    "        while self.target_player not in self.curr_obs_dict:\n",
    "            action_dict = {\n",
    "                player_id: self.opponent_agents[i-1].act(obs)\n",
    "                for i, (player_id, obs) in enumerate(self.curr_obs_dict.items())\n",
    "                if player_id != self.target_player\n",
    "            }\n",
    "            self.curr_obs_dict = self.mjx_env.step(action_dict)\n",
    "            self._update_full_info(self.curr_obs_dict)\n",
    "\n",
    "            if self.mjx_env.done(self.done_type):\n",
    "                print(f\"There are {len(self.curr_obs_dict)} players in the obs dict when the game done.\")\n",
    "                obs = list(self.curr_obs_dict.values())[0]  # 获取最后一个观察\n",
    "                feat = obs.to_features(self.feature_type)\n",
    "                done = True\n",
    "                if self.prev_obs is not None and obs is not None:\n",
    "                    reward = compute_reward(self.prev_obs, obs, self.mjx_env)\n",
    "                if self.target_player in self.curr_obs_dict:\n",
    "                    self.prev_obs = self.curr_obs_dict[self.target_player]\n",
    "                return feat, reward, done, {\"action_mask\": np.ones(181)}  # 动作掩码为 1（游戏结束）\n",
    "\n",
    "        # 处理游戏继续的情况\n",
    "        assert self.target_player in self.curr_obs_dict\n",
    "        obs = self.curr_obs_dict[self.target_player]\n",
    "        done = self.mjx_env.done(self.done_type)\n",
    "        if self.prev_obs is not None and obs is not None:\n",
    "            reward = compute_reward(self.prev_obs, obs, self.mjx_env)\n",
    "        if self.target_player in self.curr_obs_dict:\n",
    "            self.prev_obs = self.curr_obs_dict[self.target_player]\n",
    "        # feat = obs.to_features(self.feature_type)\n",
    "        if self.info_type == \"perfect\":\n",
    "            # print full info\n",
    "            if list(self.full_info.keys())[0] != 'player_0':\n",
    "                print(f\"Warning: {self.full_info.keys()} is not the same as ['player_0', 'player_1', 'player_2', 'player_3']\")\n",
    "\n",
    "            feat = np.concatenate(list(self.full_info.values()), axis=-1)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            feat = obs.to_features(self.feature_type)\n",
    "        mask = obs.action_mask()\n",
    "        \n",
    "        return feat, reward, done, {\"action_mask\": mask}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59a1868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Linear(hidden_dim, output_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base_output = self.base(x)\n",
    "        action_logits = self.actor(base_output)\n",
    "        state_values = self.critic(base_output)\n",
    "        return action_logits, state_values\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01, gamma=0.99, clip_ratio=0.4, value_coef=0.5, entropy_coef=0.01, pretrained_model = None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        \n",
    "        self.model = ActorCritic(input_dim, hidden_dim, output_dim).to(self.device)\n",
    "\n",
    "        # 如果有预训练模型，则加载它\n",
    "        if pretrained_model is not None:\n",
    "            state_dict = torch.load(pretrained_model)\n",
    "            state_dict = torch.load(\"logs/ppo_cr_cl/best_model_ppo5_stage_1.pt\")\n",
    "            state_dict['base.0.weight'] = state_dict['base.0.weight'][:, :544]\n",
    "            \n",
    "            self.model.load_state_dict(state_dict)\n",
    "            print(f\"Loaded pretrained model from {pretrained_model}\")\n",
    "            \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # 用来收集每个 episode 的数据\n",
    "        self.states = []\n",
    "        self.action_masks = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "    \n",
    "    def act(self, state, action_mask):\n",
    "        state = torch.FloatTensor(state).flatten().to(self.device)\n",
    "        mask = torch.FloatTensor(action_mask).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_logits, state_value = self.model(state)\n",
    "            action_logits = action_logits - (1 - mask) * 1e9\n",
    "            \n",
    "            dist = Categorical(logits=action_logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "        \n",
    "        self.states.append(state)\n",
    "        self.action_masks.append(mask)\n",
    "        self.actions.append(action)\n",
    "        self.values.append(state_value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self, next_state=None, done=True):\n",
    "        # 计算 returns 和 advantages\n",
    "        if not done and next_state is not None:\n",
    "            next_state = torch.FloatTensor(next_state).flatten().to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _, next_value = self.model(next_state)\n",
    "            last_value = next_value.item()\n",
    "        else:\n",
    "            last_value = 0\n",
    "        \n",
    "        states = torch.stack(self.states)\n",
    "        action_masks = torch.stack(self.action_masks)\n",
    "        actions = torch.stack(self.actions)\n",
    "        old_log_probs = torch.stack(self.log_probs)\n",
    "        old_values = torch.cat(self.values)\n",
    "        \n",
    "        returns = []\n",
    "        advantages = []\n",
    "        R = last_value\n",
    "        for r in reversed(self.rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = returns - old_values.detach()\n",
    "        \n",
    "        # Normalize advantages\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        action_logits, state_values = self.model(states)\n",
    "        \n",
    "        for i in range(len(action_logits)):\n",
    "            action_logits[i] = action_logits[i] - (1 - action_masks[i]) * 1e9\n",
    "        \n",
    "        dist = Categorical(logits=action_logits)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        ratios = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        value_loss = nn.MSELoss()(state_values.squeeze(), returns)\n",
    "        loss = actor_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 清空 episode 数据\n",
    "        self.states = []\n",
    "        self.action_masks = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': entropy.item(),\n",
    "            'total_loss': loss.item()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ef02922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards, path=\"logs/reward_curve.png\"):\n",
    "    plt.figure()\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Training Reward Curve\")\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf725ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def early_stop(logs, patience=100, delta=20):\n",
    "    \"\"\"\n",
    "    Early stopping function\n",
    "    :param logs: dict, logs of training\n",
    "    :param patience: int, number of episodes to wait before stopping\n",
    "    :param delta: int, minimum improvement to consider as progress\n",
    "    :return: bool, whether to stop training\n",
    "    \"\"\"\n",
    "    if len(logs[\"all_rewards\"]) < patience:\n",
    "        return False\n",
    "\n",
    "    recent_rewards = logs[\"all_rewards\"][-patience:]\n",
    "    best_before = max(logs[\"all_rewards\"][:-patience]) if len(logs[\"all_rewards\"]) > patience else float('-inf')\n",
    "    return max(recent_rewards) < best_before + delta\n",
    "\n",
    "\n",
    "def train_curriculum_agent(\n",
    "    info_type = \"default\", opponents = list(RandomAgent() for _ in range(3)),\n",
    "    num_episodes=1000, log_interval = 100,\n",
    "    pretrained_model=None,\n",
    "    stage = 1,\n",
    "    all_logs = {\n",
    "        \"all_rewards\": [],\n",
    "        \"all_actor_loss\": [],\n",
    "        \"all_value_loss\": [],\n",
    "    },\n",
    "    \n",
    "    patience=100,\n",
    "    delta=20,\n",
    "):\n",
    "    env = GymEnv(opponent_agents=opponents, info_type=info_type)\n",
    "    obs, info = env.reset()\n",
    "    obs_shape = obs.flatten().shape[0]\n",
    "    action_dim = len(info[\"action_mask\"])\n",
    "    agent = PPOAgent(\n",
    "        input_dim=obs_shape,\n",
    "        hidden_dim=128,\n",
    "        output_dim=action_dim,\n",
    "        pretrained_model=pretrained_model, # If have a pretrained model, load it\n",
    "    )\n",
    "\n",
    "    rolling_rewards = []\n",
    "    best_reward = -np.inf\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(obs, info[\"action_mask\"])\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            agent.store_reward(reward)\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "\n",
    "        stats = agent.update(next_obs, done)\n",
    "        all_logs[\"all_actor_loss\"].append(stats[\"actor_loss\"])\n",
    "        all_logs[\"all_value_loss\"].append(stats[\"value_loss\"])\n",
    "        all_logs[\"all_rewards\"].append(total_reward)\n",
    "\n",
    "        if episode % log_interval == 0:\n",
    "            avg_reward = np.mean(all_logs[\"all_rewards\"][-log_interval:])\n",
    "            rolling_rewards.append(avg_reward)\n",
    "            print(f\"Episode {episode}/{num_episodes}, avg reward: {avg_reward:.3f}\")\n",
    "            # 更新最佳模型\n",
    "            if avg_reward > best_reward:\n",
    "                best_reward = avg_reward\n",
    "                torch.save(agent.model.state_dict(), f\"logs/ppo_cr_cl/best_model_ppo5_stage_{stage}.pt\")\n",
    "                print(f\"Best model saved with reward: {best_reward:.3f}\")\n",
    "            \n",
    "            log_data = {\n",
    "                \"episode\": episode,\n",
    "                \"avg_reward\": avg_reward,\n",
    "                \"actor_loss\": stats[\"actor_loss\"],\n",
    "                \"value_loss\": stats[\"value_loss\"],\n",
    "                \"entropy\": stats[\"entropy\"],\n",
    "                \"total_loss\": stats[\"total_loss\"],\n",
    "            }\n",
    "\n",
    "            with open(\"logs/ppo_cr_cl/stage_{stage}_logs.json\", \"a\") as f:\n",
    "                f.write(json.dumps(log_data) + \"\\n\")\n",
    "            print(f\"Episode {episode} logs saved.\")\n",
    "            plot_rewards(rolling_rewards, path=f\"logs/ppo_cr_cl/stage_{stage}_reward_curve.png\")\n",
    "\n",
    "            if early_stop(all_logs, patience=patience, delta=delta):\n",
    "                print(f\"Early stopping triggered at episode {episode}. Best avg reward: {best_reward:.3f}\")\n",
    "                break\n",
    "    return agent, all_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03117e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Stage 1: Perfect Information with Random Agent Opponents\n",
      "There are 4 players in the obs dict when the game done.\n",
      "Episode 1/1, avg reward: 313.000\n",
      "Best model saved with reward: 313.000\n",
      "Episode 1 logs saved.\n",
      "Training Stage 2: Imperfect Information with Shanten Agent Opponents\n",
      "Loaded pretrained model from logs/ppo_cr_cl/best_model_ppo5_stage_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4114520/882516935.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(pretrained_model)\n",
      "/tmp/ipykernel_4114520/882516935.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"logs/ppo_cr_cl/best_model_ppo5_stage_1.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 players in the obs dict when the game done.\n",
      "Episode 1/1, avg reward: 520.000\n",
      "Best model saved with reward: 520.000\n",
      "Episode 1 logs saved.\n",
      "Training Stage 3: Imperfect Information with Shanten Agent Opponents\n",
      "Loaded pretrained model from logs/ppo_cr_cl/best_model_ppo5_stage_2.pt\n",
      "There are 4 players in the obs dict when the game done.\n",
      "Episode 1/1, avg reward: 66.000\n",
      "Best model saved with reward: 66.000\n",
      "Episode 1 logs saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Stage 1: Perfect Information with Random Agent Opponents\")\n",
    "\n",
    "stage_1_agent, all_logs = train_curriculum_agent(\n",
    "    info_type=\"perfect\",\n",
    "    opponents=[RandomAgent() for _ in range(3)],\n",
    "    num_episodes=1000,\n",
    "    log_interval=100,\n",
    "    pretrained_model=None,\n",
    "    stage=1\n",
    ")\n",
    "\n",
    "print(\"Training Stage 2: Imperfect Information with Shanten Agent Opponents\")\n",
    "stage_2_agent, all_logs = train_curriculum_agent(\n",
    "    info_type=\"default\",\n",
    "    opponents=[RandomAgent() for _ in range(3)],\n",
    "    num_episodes=1500,\n",
    "    log_interval=100,\n",
    "    pretrained_model=\"logs/ppo_cr_cl/best_model_ppo5_stage_1.pt\",\n",
    "    stage=2,\n",
    "    all_logs=all_logs,\n",
    "    patience=100,\n",
    "    delta=20,\n",
    ")\n",
    "\n",
    "print(\"Training Stage 3: Imperfect Information with Shanten Agent Opponents\")\n",
    "stage_3_agent, all_logs = train_curriculum_agent(\n",
    "    info_type=\"default\",\n",
    "    opponents=[ShantenAgent() for _ in range(3)],\n",
    "    num_episodes=2000,\n",
    "    log_interval=100,\n",
    "    pretrained_model=\"logs/ppo_cr_cl/best_model_ppo5_stage_2.pt\",\n",
    "    stage=3,\n",
    "    all_logs=all_logs,\n",
    "    patience=100,\n",
    "    delta=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfede449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Stage 4: Imperfect Information with Trained Agent Opponents\")\n",
    "stage_4_agent, all_logs = train_curriculum_agent(\n",
    "    info_type=\"default\",\n",
    "    opponents=[stage_3_agent for _ in range(3)],\n",
    "    num_episodes=1000,\n",
    "    log_interval=100,\n",
    "    pretrained_model=\"logs/ppo_cr_cl/best_model_ppo5_stage_3.pt\",\n",
    "    stage=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa583c",
   "metadata": {},
   "source": [
    "课程学习+PPO+自定义奖励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ccfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5000\n",
      "Episode 1 finished with total reward: 145.000\n",
      "Episode 2/5000\n",
      "Episode 2 finished with total reward: 239.000\n",
      "Episode 3/5000\n",
      "Episode 3 finished with total reward: 27.000\n",
      "Episode 4/5000\n",
      "Episode 4 finished with total reward: 336.000\n",
      "Episode 5/5000\n",
      "Episode 5 finished with total reward: 213.000\n",
      "Episode 6/5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     48\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(obs, info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 49\u001b[0m     next_obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstore_reward(reward)\n\u001b[1;32m     51\u001b[0m     obs \u001b[38;5;241m=\u001b[39m next_obs\n",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m, in \u001b[0;36mGymEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_player \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_obs_dict:\n\u001b[1;32m     98\u001b[0m     action_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     99\u001b[0m         player_id: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopponent_agents[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mact(obs)\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, (player_id, obs) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_obs_dict\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m player_id \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_player\n\u001b[1;32m    102\u001b[0m     }\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_obs_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmjx_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_full_info(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_obs_dict)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmjx_env\u001b[38;5;241m.\u001b[39mdone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_type):\n",
      "File \u001b[0;32m~/mjx/mjx/env.py:27\u001b[0m, in \u001b[0;36mMjxEnv.step\u001b[0;34m(self, aciton_dict)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, aciton_dict: Dict[\u001b[38;5;28mstr\u001b[39m, Action]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Observation]:\n\u001b[1;32m     26\u001b[0m     cpp_action_dict: Dict[\u001b[38;5;28mstr\u001b[39m, _mjx\u001b[38;5;241m.\u001b[39mAction] \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39m_cpp_obj \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m aciton_dict\u001b[38;5;241m.\u001b[39mitems()}  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     cpp_obs_dict: Dict[\u001b[38;5;28mstr\u001b[39m, _mjx\u001b[38;5;241m.\u001b[39mObservation] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_action_dict\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: Observation\u001b[38;5;241m.\u001b[39m_from_cpp_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m cpp_obs_dict\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Initialize the environment: Random opponent agent\n",
    "opponents = [RandomAgent() for _ in range(3)]  \n",
    "env = GymEnv(opponent_agents=opponents, info_type=\"perfect\")\n",
    "\n",
    "# 获取 observation shape 和 action space\n",
    "obs, info = env.reset()\n",
    "obs_shape = obs.flatten().shape[0]\n",
    "action_dim = len(info[\"action_mask\"])  # 动作数量（181）\n",
    "\n",
    "# 初始化 PPO agent\n",
    "agent = PPOAgent(\n",
    "    input_dim=obs_shape,\n",
    "    hidden_dim=128,\n",
    "    output_dim=action_dim\n",
    ")\n",
    "\n",
    "# 训练超参数\n",
    "num_episodes = 5000\n",
    "log_interval = 100  # 每 N 个 episode 记录一次\n",
    "rolling_rewards = []\n",
    "\n",
    "all_rewards = []\n",
    "all_actor_loss = []\n",
    "all_value_loss = []\n",
    "best_reward = -float(\"inf\")\n",
    "\n",
    "\n",
    "# \n",
    "for episode in range(1, num_episodes + 1):\n",
    "    print(f\"Episode {episode}/{num_episodes}\")\n",
    "    # Add Curriculum Learning\n",
    "\n",
    "    # if episode == 1000:\n",
    "    # Stage 2: imperfect information with Random agent opponents\n",
    "    if episode == 2:\n",
    "        # Parameter transfer + fine tuning\n",
    "        torch.save(agent.model.state_dict(), \"logs/ppo_cr_cl/best_model_ppo5_stage1.pt\")\n",
    "        agent.model.load_state_dict(torch.load(\"logs/ppo_cr_cl/best_model_ppo5_stage1.pt\"))\n",
    "\n",
    "        env._set_info_type(\"default\")\n",
    "    \n",
    "    if episode == 3:\n",
    "        env._set_opponent_agents([ShantenAgent() for _ in range(3)])\n",
    "    \n",
    "    # if episode == 4:\n",
    "        # env._set_opponent_agents([agent for _ in range(3)])\n",
    "    \n",
    "        \n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(obs, info[\"action_mask\"])\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        agent.store_reward(reward)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    \n",
    "    stats = agent.update()\n",
    "    all_rewards.append(total_reward)\n",
    "    all_actor_loss.append(stats['actor_loss'])\n",
    "    all_value_loss.append(stats['value_loss'])\n",
    "    print(f\"Episode {episode} finished with total reward: {total_reward:.3f}\")\n",
    "\n",
    "    # 打印日志\n",
    "    if episode % log_interval == 0:\n",
    "        avg_reward = np.mean(all_rewards[-log_interval:])\n",
    "        rolling_rewards.append(avg_reward)\n",
    "        print(f\"Episode {episode}/{num_episodes}, avg reward: {avg_reward:.3f}\")\n",
    "        # 更新最佳模型\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            torch.save(agent.model.state_dict(), \"best_model_ppo5.pt\")\n",
    "            print(f\"Best model saved with reward: {best_reward:.3f}\")\n",
    "\n",
    "        log_data = {\n",
    "            'episode': episode,\n",
    "            'avg_reward': avg_reward,\n",
    "            'actor_loss': stats['actor_loss'],\n",
    "            'value_loss': stats['value_loss'],\n",
    "            'entropy': stats['entropy'],\n",
    "            'total_loss': stats['total_loss']\n",
    "        }\n",
    "        with open('logs/ppo_cr_cl/training_log.json', 'a') as f:\n",
    "            json.dump(log_data, f)\n",
    "            f.write('\\n')\n",
    "        # 绘制训练曲线\n",
    "        plot_rewards(rolling_rewards, path=f\"logs/ppo_cr_cl/reward_curve_{episode}.png\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
