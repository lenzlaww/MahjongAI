{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831bf3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import mjx\n",
    "from mjx.agents import RandomAgent, ShantenAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d674713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import mjx\n",
    "from mjx.agents import RandomAgent, ShantenAgent  # 使用 Shanten agent\n",
    "\n",
    "class GymEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self, \n",
    "        opponent_agents: list, \n",
    "        reward_type: str = \"game_tenhou_7dan\", \n",
    "        done_type: str = \"game\", \n",
    "        feature_type: str = \"mjx-small-v0\"\n",
    "    ):\n",
    "        super(GymEnv, self).__init__()\n",
    "        self.opponent_agents = opponent_agents  # 对手列表，包含 Shanten agent\n",
    "        self.reward_type = reward_type\n",
    "        self.done_type = done_type\n",
    "        self.feature_type = feature_type\n",
    "\n",
    "        self.target_player = \"player_0\"  # 我方玩家\n",
    "        self.mjx_env = mjx.MjxEnv()\n",
    "        self.curr_obs_dict = self.mjx_env.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"环境重置，返回第一个状态和可用动作\"\"\"\n",
    "        self.curr_obs_dict = self.mjx_env.reset()\n",
    "\n",
    "        # 跳过其他玩家的回合，直到轮到我们的玩家\n",
    "        while self.target_player not in self.curr_obs_dict:\n",
    "            action_dict = {\n",
    "                player_id: self.opponent_agents[i].act(obs)\n",
    "                for i, (player_id, obs) in enumerate(self.curr_obs_dict.items())\n",
    "            }\n",
    "            self.curr_obs_dict = self.mjx_env.step(action_dict)\n",
    "\n",
    "        # 返回当前玩家的特征\n",
    "        obs = self.curr_obs_dict[self.target_player]\n",
    "        feat = obs.to_features(self.feature_type)\n",
    "        mask = obs.action_mask()  # 获取有效动作\n",
    "        return feat, {\"action_mask\": mask}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"执行一个步骤，并返回下一个状态、奖励、是否结束\"\"\"\n",
    "        action_dict = {self.target_player: mjx.Action.select_from(action, self.curr_obs_dict[self.target_player].legal_actions())}\n",
    "\n",
    "        # 遍历当前所有玩家\n",
    "        for i, (pid, obs) in enumerate(self.curr_obs_dict.items()):\n",
    "            if pid != self.target_player:\n",
    "                # 需要将 i-1 映射到对手代理列表\n",
    "                action_dict[pid] = self.opponent_agents[i-1].act(obs)\n",
    "\n",
    "        # 更新状态\n",
    "        self.curr_obs_dict = self.mjx_env.step(action_dict)\n",
    "\n",
    "        # 检查是否是当前玩家的回合\n",
    "        while self.target_player not in self.curr_obs_dict:\n",
    "            action_dict = {\n",
    "                player_id: self.opponent_agents[i-1].act(obs)\n",
    "                for i, (player_id, obs) in enumerate(self.curr_obs_dict.items())\n",
    "                if player_id != self.target_player\n",
    "            }\n",
    "            self.curr_obs_dict = self.mjx_env.step(action_dict)\n",
    "\n",
    "            if self.mjx_env.done(self.done_type):\n",
    "                obs = list(self.curr_obs_dict.values())[0]  # 获取最后一个观察\n",
    "                feat = obs.to_features(self.feature_type)\n",
    "                done = True\n",
    "                reward = self.mjx_env.rewards(self.reward_type)[self.target_player]\n",
    "                return feat, reward, done, {\"action_mask\": np.ones(181)}  # 动作掩码为 1（游戏结束）\n",
    "\n",
    "        # 处理游戏继续的情况\n",
    "        assert self.target_player in self.curr_obs_dict\n",
    "        obs = self.curr_obs_dict[self.target_player]\n",
    "        done = self.mjx_env.done(self.done_type)\n",
    "        reward = self.mjx_env.rewards(self.reward_type)[self.target_player]\n",
    "        feat = obs.to_features(self.feature_type)\n",
    "        mask = obs.action_mask()\n",
    "\n",
    "        return feat, reward, done, {\"action_mask\": mask}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a1868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Linear(hidden_dim, output_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base_output = self.base(x)\n",
    "        action_logits = self.actor(base_output)\n",
    "        state_values = self.critic(base_output)\n",
    "        return action_logits, state_values\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01, gamma=0.99, clip_ratio=0.4, value_coef=0.5, entropy_coef=0.01):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic(input_dim, hidden_dim, output_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # 用来收集每个 episode 的数据\n",
    "        self.states = []\n",
    "        self.action_masks = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "    \n",
    "    def act(self, state, action_mask):\n",
    "        state = torch.FloatTensor(state).flatten().to(self.device)\n",
    "        mask = torch.FloatTensor(action_mask).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_logits, state_value = self.model(state)\n",
    "            action_logits = action_logits - (1 - mask) * 1e9\n",
    "            \n",
    "            dist = Categorical(logits=action_logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "        \n",
    "        self.states.append(state)\n",
    "        self.action_masks.append(mask)\n",
    "        self.actions.append(action)\n",
    "        self.values.append(state_value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self, next_state=None, done=True):\n",
    "        # 计算 returns 和 advantages\n",
    "        if not done and next_state is not None:\n",
    "            next_state = torch.FloatTensor(next_state).flatten().to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _, next_value = self.model(next_state)\n",
    "            last_value = next_value.item()\n",
    "        else:\n",
    "            last_value = 0\n",
    "        \n",
    "        states = torch.stack(self.states)\n",
    "        action_masks = torch.stack(self.action_masks)\n",
    "        actions = torch.stack(self.actions)\n",
    "        old_log_probs = torch.stack(self.log_probs)\n",
    "        old_values = torch.cat(self.values)\n",
    "        \n",
    "        returns = []\n",
    "        advantages = []\n",
    "        R = last_value\n",
    "        for r in reversed(self.rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = returns - old_values.detach()\n",
    "        \n",
    "        # Normalize advantages\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        action_logits, state_values = self.model(states)\n",
    "        \n",
    "        for i in range(len(action_logits)):\n",
    "            action_logits[i] = action_logits[i] - (1 - action_masks[i]) * 1e9\n",
    "        \n",
    "        dist = Categorical(logits=action_logits)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        ratios = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        value_loss = nn.MSELoss()(state_values.squeeze(), returns)\n",
    "        loss = actor_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 清空 episode 数据\n",
    "        self.states = []\n",
    "        self.action_masks = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': entropy.item(),\n",
    "            'total_loss': loss.item()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fe17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化环境（Shanten agent 作为对手）\n",
    "opponents = [ShantenAgent() for _ in range(3)]  # 假设我们的 agent 是 player_0，剩下三个是对手\n",
    "env = GymEnv(opponent_agents=opponents)\n",
    "\n",
    "# 获取 observation shape 和 action space\n",
    "obs, info = env.reset()\n",
    "obs_shape = obs.flatten().shape[0]\n",
    "action_dim = len(info[\"action_mask\"])  # 动作数量（181）\n",
    "\n",
    "# 初始化 PPO agent\n",
    "agent = PPOAgent(\n",
    "    input_dim=obs_shape,\n",
    "    hidden_dim=128,\n",
    "    output_dim=action_dim\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef02922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards, path=\"logs/reward_curve.png\"):\n",
    "    plt.figure()\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Training Reward Curve\")\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ccfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/3000, avg reward: -127.350\n",
      "Best model saved with reward: -127.350\n",
      "Episode 200/3000, avg reward: -128.250\n",
      "Episode 300/3000, avg reward: -125.550\n",
      "Best model saved with reward: -125.550\n",
      "Episode 400/3000, avg reward: -129.600\n",
      "Episode 500/3000, avg reward: -122.850\n",
      "Best model saved with reward: -122.850\n",
      "Episode 600/3000, avg reward: -120.150\n",
      "Best model saved with reward: -120.150\n",
      "Episode 700/3000, avg reward: -132.300\n",
      "Episode 800/3000, avg reward: -122.400\n",
      "Episode 900/3000, avg reward: -122.400\n",
      "Episode 1000/3000, avg reward: -130.950\n",
      "Episode 1100/3000, avg reward: -123.750\n",
      "Episode 1200/3000, avg reward: -126.900\n",
      "Episode 1300/3000, avg reward: -124.200\n",
      "Episode 1400/3000, avg reward: -128.250\n",
      "Episode 1500/3000, avg reward: -127.800\n",
      "Episode 1600/3000, avg reward: -123.750\n",
      "Episode 1700/3000, avg reward: -123.750\n",
      "Episode 1800/3000, avg reward: -125.100\n",
      "Episode 1900/3000, avg reward: -123.750\n",
      "Episode 2000/3000, avg reward: -129.150\n",
      "Episode 2100/3000, avg reward: -130.950\n",
      "Episode 2200/3000, avg reward: -122.850\n",
      "Episode 2300/3000, avg reward: -129.600\n",
      "Episode 2400/3000, avg reward: -128.250\n",
      "Episode 2500/3000, avg reward: -123.750\n",
      "Episode 2600/3000, avg reward: -125.550\n",
      "Episode 2700/3000, avg reward: -130.950\n",
      "Episode 2800/3000, avg reward: -124.200\n",
      "Episode 2900/3000, avg reward: -128.250\n",
      "Episode 3000/3000, avg reward: -125.550\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 训练超参数\n",
    "num_episodes = 3000\n",
    "log_interval = 100  # 每 N 个 episode 记录一次\n",
    "rolling_rewards = []\n",
    "\n",
    "all_rewards = []\n",
    "all_actor_loss = []\n",
    "all_value_loss = []\n",
    "best_reward = -float(\"inf\")\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(obs, info[\"action_mask\"])\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        agent.store_reward(reward)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    \n",
    "    stats = agent.update()\n",
    "    all_rewards.append(total_reward)\n",
    "    all_actor_loss.append(stats['actor_loss'])\n",
    "    all_value_loss.append(stats['value_loss'])\n",
    "\n",
    "    # 打印日志\n",
    "    if episode % log_interval == 0:\n",
    "        avg_reward = np.mean(all_rewards[-log_interval:])\n",
    "        rolling_rewards.append(avg_reward)\n",
    "        print(f\"Episode {episode}/{num_episodes}, avg reward: {avg_reward:.3f}\")\n",
    "        # 更新最佳模型\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            torch.save(agent.model.state_dict(), \"logs/ppo/ppo_model.pt\")\n",
    "            print(f\"Best model saved with reward: {best_reward:.3f}\")\n",
    "\n",
    "        log_data = {\n",
    "            'episode': episode,\n",
    "            'avg_reward': avg_reward,\n",
    "            'actor_loss': stats['actor_loss'],\n",
    "            'value_loss': stats['value_loss'],\n",
    "            'entropy': stats['entropy'],\n",
    "            'total_loss': stats['total_loss']\n",
    "        }\n",
    "        with open('logs/ppo/training_log.json', 'a') as f:\n",
    "            json.dump(log_data, f)\n",
    "            f.write('\\n')\n",
    "# 绘制训练曲线\n",
    "plot_rewards(rolling_rewards, path=f\"logs/ppo/reward_curve_{episode}.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "588e9f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-120.15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
