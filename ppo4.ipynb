{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831bf3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import mjx\n",
    "from mjx.agents import RandomAgent, ShantenAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d674713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import mjx\n",
    "from mjx.agents import RandomAgent, ShantenAgent  # 使用 Shanten agent\n",
    "from utils import compute_reward\n",
    "\n",
    "class GymEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self, \n",
    "        opponent_agents: list, \n",
    "        reward_type: str = \"game_tenhou_7dan\", \n",
    "        done_type: str = \"game\", \n",
    "        feature_type: str = \"mjx-small-v0\"\n",
    "    ):\n",
    "        super(GymEnv, self).__init__()\n",
    "        self.opponent_agents = opponent_agents  # 对手列表，包含 Shanten agent\n",
    "        self.reward_type = reward_type\n",
    "        self.done_type = done_type\n",
    "        self.feature_type = feature_type\n",
    "\n",
    "        self.target_player = \"player_0\"  # 我方玩家\n",
    "        self.mjx_env = mjx.MjxEnv()\n",
    "        self.curr_obs_dict = self.mjx_env.reset()\n",
    "\n",
    "        self.prev_obs = None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"环境重置，返回第一个状态和可用动作\"\"\"\n",
    "        self.curr_obs_dict = self.mjx_env.reset()\n",
    "        self.prev_obs = None\n",
    "\n",
    "        # 跳过其他玩家的回合，直到轮到我们的玩家\n",
    "        while self.target_player not in self.curr_obs_dict:\n",
    "            action_dict = {\n",
    "                player_id: self.opponent_agents[i].act(obs)\n",
    "                for i, (player_id, obs) in enumerate(self.curr_obs_dict.items())\n",
    "            }\n",
    "            self.curr_obs_dict = self.mjx_env.step(action_dict)\n",
    "\n",
    "        # 返回当前玩家的特征\n",
    "        obs = self.curr_obs_dict[self.target_player]\n",
    "        feat = obs.to_features(self.feature_type)\n",
    "        mask = obs.action_mask()  # 获取有效动作\n",
    "        return feat, {\"action_mask\": mask}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"执行一个步骤，并返回下一个状态、奖励、是否结束\"\"\"\n",
    "        action_dict = {self.target_player: mjx.Action.select_from(action, self.curr_obs_dict[self.target_player].legal_actions())}\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # 遍历当前所有玩家\n",
    "        for i, (pid, obs) in enumerate(self.curr_obs_dict.items()):\n",
    "            if pid != self.target_player:\n",
    "                # 需要将 i-1 映射到对手代理列表\n",
    "                action_dict[pid] = self.opponent_agents[i-1].act(obs)\n",
    "\n",
    "        # 更新状态\n",
    "        self.curr_obs_dict = self.mjx_env.step(action_dict)\n",
    "        \n",
    "\n",
    "        # 检查是否是当前玩家的回合\n",
    "        while self.target_player not in self.curr_obs_dict:\n",
    "            action_dict = {\n",
    "                player_id: self.opponent_agents[i-1].act(obs)\n",
    "                for i, (player_id, obs) in enumerate(self.curr_obs_dict.items())\n",
    "                if player_id != self.target_player\n",
    "            }\n",
    "            self.curr_obs_dict = self.mjx_env.step(action_dict)\n",
    "\n",
    "            if self.mjx_env.done(self.done_type):\n",
    "                obs = list(self.curr_obs_dict.values())[0]  # 获取最后一个观察\n",
    "                feat = obs.to_features(self.feature_type)\n",
    "                done = True\n",
    "                if self.prev_obs is not None and obs is not None:\n",
    "                    reward = compute_reward(self.prev_obs, obs, self.mjx_env)\n",
    "                if self.target_player in self.curr_obs_dict:\n",
    "                    self.prev_obs = self.curr_obs_dict[self.target_player]\n",
    "                return feat, reward, done, {\"action_mask\": np.ones(181)}  # 动作掩码为 1（游戏结束）\n",
    "\n",
    "        # 处理游戏继续的情况\n",
    "        assert self.target_player in self.curr_obs_dict\n",
    "        obs = self.curr_obs_dict[self.target_player]\n",
    "        done = self.mjx_env.done(self.done_type)\n",
    "        if self.prev_obs is not None and obs is not None:\n",
    "            reward = compute_reward(self.prev_obs, obs, self.mjx_env)\n",
    "        if self.target_player in self.curr_obs_dict:\n",
    "            self.prev_obs = self.curr_obs_dict[self.target_player]\n",
    "        feat = obs.to_features(self.feature_type)\n",
    "        mask = obs.action_mask()\n",
    "\n",
    "        return feat, reward, done, {\"action_mask\": mask}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Linear(hidden_dim, output_dim)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base_output = self.base(x)\n",
    "        action_logits = self.actor(base_output)\n",
    "        state_values = self.critic(base_output)\n",
    "        return action_logits, state_values\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=1e-4, gamma=0.99, clip_ratio=0.4, value_coef=0.5, entropy_coef=0.01):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = ActorCritic(input_dim, hidden_dim, output_dim).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # 用来收集每个 episode 的数据\n",
    "        self.states = []\n",
    "        self.action_masks = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "    \n",
    "    def act(self, state, action_mask):\n",
    "        state = torch.FloatTensor(state).flatten().to(self.device)\n",
    "        mask = torch.FloatTensor(action_mask).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_logits, state_value = self.model(state)\n",
    "            action_logits = action_logits - (1 - mask) * 1e9\n",
    "            \n",
    "            dist = Categorical(logits=action_logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "        \n",
    "        self.states.append(state)\n",
    "        self.action_masks.append(mask)\n",
    "        self.actions.append(action)\n",
    "        self.values.append(state_value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self, next_state=None, done=True):\n",
    "        # 计算 returns 和 advantages\n",
    "        if not done and next_state is not None:\n",
    "            next_state = torch.FloatTensor(next_state).flatten().to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _, next_value = self.model(next_state)\n",
    "            last_value = next_value.item()\n",
    "        else:\n",
    "            last_value = 0\n",
    "        \n",
    "        states = torch.stack(self.states)\n",
    "        action_masks = torch.stack(self.action_masks)\n",
    "        actions = torch.stack(self.actions)\n",
    "        old_log_probs = torch.stack(self.log_probs)\n",
    "        old_values = torch.cat(self.values)\n",
    "        \n",
    "        returns = []\n",
    "        advantages = []\n",
    "        R = last_value\n",
    "        for r in reversed(self.rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns).to(self.device)\n",
    "        advantages = returns - old_values.detach()\n",
    "        \n",
    "        # Normalize advantages\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        action_logits, state_values = self.model(states)\n",
    "        \n",
    "        for i in range(len(action_logits)):\n",
    "            action_logits[i] = action_logits[i] - (1 - action_masks[i]) * 1e9\n",
    "        \n",
    "        dist = Categorical(logits=action_logits)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        ratios = torch.exp(new_log_probs - old_log_probs.detach())\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1.0 - self.clip_ratio, 1.0 + self.clip_ratio) * advantages\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        value_loss = nn.MSELoss()(state_values.squeeze(), returns)\n",
    "        loss = actor_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 清空 episode 数据\n",
    "        self.states = []\n",
    "        self.action_masks = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': entropy.item(),\n",
    "            'total_loss': loss.item()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化环境（Shanten agent 作为对手）\n",
    "opponents = [ShantenAgent() for _ in range(3)]  # 假设我们的 agent 是 player_0，剩下三个是对手\n",
    "env = GymEnv(opponent_agents=opponents)\n",
    "\n",
    "# 获取 observation shape 和 action space\n",
    "obs, info = env.reset()\n",
    "obs_shape = obs.flatten().shape[0]\n",
    "action_dim = len(info[\"action_mask\"])  # 动作数量（181）\n",
    "\n",
    "# 初始化 PPO agent\n",
    "agent = PPOAgent(\n",
    "    input_dim=obs_shape,\n",
    "    hidden_dim=128,\n",
    "    output_dim=action_dim,\n",
    "    lr = 1e-4,              # small learning rate\n",
    "    entropy_coef=0.001      # small entropy coefficient: small curiousity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef02922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards, path=\"logs/reward_curve.png\"):\n",
    "    plt.figure()\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Training Reward Curve\")\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047ccfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/3000\n",
      "Episode 1 finished with total reward: 175.000\n",
      "Episode 2/3000\n",
      "Episode 2 finished with total reward: 35.000\n",
      "Episode 3/3000\n",
      "Episode 3 finished with total reward: 139.000\n",
      "Episode 4/3000\n",
      "Episode 4 finished with total reward: 96.000\n",
      "Episode 5/3000\n",
      "Episode 5 finished with total reward: 151.000\n",
      "Episode 6/3000\n",
      "Episode 6 finished with total reward: 390.000\n",
      "Episode 7/3000\n",
      "Episode 7 finished with total reward: 221.000\n",
      "Episode 8/3000\n",
      "Episode 8 finished with total reward: 126.000\n",
      "Episode 9/3000\n",
      "Episode 9 finished with total reward: 263.000\n",
      "Episode 10/3000\n",
      "Episode 10 finished with total reward: 40.000\n",
      "Episode 11/3000\n",
      "Episode 11 finished with total reward: 91.000\n",
      "Episode 12/3000\n",
      "Episode 12 finished with total reward: -76.000\n",
      "Episode 13/3000\n",
      "Episode 13 finished with total reward: 247.000\n",
      "Episode 14/3000\n",
      "Episode 14 finished with total reward: 107.000\n",
      "Episode 15/3000\n",
      "Episode 15 finished with total reward: 66.000\n",
      "Episode 16/3000\n",
      "Episode 16 finished with total reward: 47.000\n",
      "Episode 17/3000\n",
      "Episode 17 finished with total reward: 10.000\n",
      "Episode 18/3000\n",
      "Episode 18 finished with total reward: 210.000\n",
      "Episode 19/3000\n",
      "Episode 19 finished with total reward: 241.000\n",
      "Episode 20/3000\n",
      "Episode 20 finished with total reward: 199.000\n",
      "Episode 21/3000\n",
      "Episode 21 finished with total reward: 39.000\n",
      "Episode 22/3000\n",
      "Episode 22 finished with total reward: 89.000\n",
      "Episode 23/3000\n",
      "Episode 23 finished with total reward: 80.000\n",
      "Episode 24/3000\n",
      "Episode 24 finished with total reward: 255.000\n",
      "Episode 25/3000\n",
      "Episode 25 finished with total reward: -2.000\n",
      "Episode 26/3000\n",
      "Episode 26 finished with total reward: 183.000\n",
      "Episode 27/3000\n",
      "Episode 27 finished with total reward: 278.000\n",
      "Episode 28/3000\n",
      "Episode 28 finished with total reward: -75.000\n",
      "Episode 29/3000\n",
      "Episode 29 finished with total reward: 331.000\n",
      "Episode 30/3000\n",
      "Episode 30 finished with total reward: 98.000\n",
      "Episode 31/3000\n",
      "Episode 31 finished with total reward: 177.000\n",
      "Episode 32/3000\n",
      "Episode 32 finished with total reward: 26.000\n",
      "Episode 33/3000\n",
      "Episode 33 finished with total reward: 391.000\n",
      "Episode 34/3000\n",
      "Episode 34 finished with total reward: 98.000\n",
      "Episode 35/3000\n",
      "Episode 35 finished with total reward: 170.000\n",
      "Episode 36/3000\n",
      "Episode 36 finished with total reward: -35.000\n",
      "Episode 37/3000\n",
      "Episode 37 finished with total reward: 42.000\n",
      "Episode 38/3000\n",
      "Episode 38 finished with total reward: 39.000\n",
      "Episode 39/3000\n",
      "Episode 39 finished with total reward: 91.000\n",
      "Episode 40/3000\n",
      "Episode 40 finished with total reward: 75.000\n",
      "Episode 41/3000\n",
      "Episode 41 finished with total reward: -5.000\n",
      "Episode 42/3000\n",
      "Episode 42 finished with total reward: 158.000\n",
      "Episode 43/3000\n",
      "Episode 43 finished with total reward: 307.000\n",
      "Episode 44/3000\n",
      "Episode 44 finished with total reward: 426.000\n",
      "Episode 45/3000\n",
      "Episode 45 finished with total reward: 360.000\n",
      "Episode 46/3000\n",
      "Episode 46 finished with total reward: 178.000\n",
      "Episode 47/3000\n",
      "Episode 47 finished with total reward: 47.000\n",
      "Episode 48/3000\n",
      "Episode 48 finished with total reward: 129.000\n",
      "Episode 49/3000\n",
      "Episode 49 finished with total reward: -12.000\n",
      "Episode 50/3000\n",
      "Episode 50 finished with total reward: 291.000\n",
      "Episode 51/3000\n",
      "Episode 51 finished with total reward: 81.000\n",
      "Episode 52/3000\n",
      "Episode 52 finished with total reward: 175.000\n",
      "Episode 53/3000\n",
      "Episode 53 finished with total reward: 180.000\n",
      "Episode 54/3000\n",
      "Episode 54 finished with total reward: 299.000\n",
      "Episode 55/3000\n",
      "Episode 55 finished with total reward: 395.000\n",
      "Episode 56/3000\n",
      "Episode 56 finished with total reward: 52.000\n",
      "Episode 57/3000\n",
      "Episode 57 finished with total reward: 103.000\n",
      "Episode 58/3000\n",
      "Episode 58 finished with total reward: 350.000\n",
      "Episode 59/3000\n",
      "Episode 59 finished with total reward: -135.000\n",
      "Episode 60/3000\n",
      "Episode 60 finished with total reward: 338.000\n",
      "Episode 61/3000\n",
      "Episode 61 finished with total reward: 217.000\n",
      "Episode 62/3000\n",
      "Episode 62 finished with total reward: -101.000\n",
      "Episode 63/3000\n",
      "Episode 63 finished with total reward: 223.000\n",
      "Episode 64/3000\n",
      "Episode 64 finished with total reward: 271.000\n",
      "Episode 65/3000\n",
      "Episode 65 finished with total reward: 282.000\n",
      "Episode 66/3000\n",
      "Episode 66 finished with total reward: -57.000\n",
      "Episode 67/3000\n",
      "Episode 67 finished with total reward: -11.000\n",
      "Episode 68/3000\n",
      "Episode 68 finished with total reward: 139.000\n",
      "Episode 69/3000\n",
      "Episode 69 finished with total reward: 221.000\n",
      "Episode 70/3000\n",
      "Episode 70 finished with total reward: 301.000\n",
      "Episode 71/3000\n",
      "Episode 71 finished with total reward: -215.000\n",
      "Episode 72/3000\n",
      "Episode 72 finished with total reward: -57.000\n",
      "Episode 73/3000\n",
      "Episode 73 finished with total reward: 104.000\n",
      "Episode 74/3000\n",
      "Episode 74 finished with total reward: 36.000\n",
      "Episode 75/3000\n",
      "Episode 75 finished with total reward: 132.000\n",
      "Episode 76/3000\n",
      "Episode 76 finished with total reward: 160.000\n",
      "Episode 77/3000\n",
      "Episode 77 finished with total reward: -30.000\n",
      "Episode 78/3000\n",
      "Episode 78 finished with total reward: 119.000\n",
      "Episode 79/3000\n",
      "Episode 79 finished with total reward: -23.000\n",
      "Episode 80/3000\n",
      "Episode 80 finished with total reward: -57.000\n",
      "Episode 81/3000\n",
      "Episode 81 finished with total reward: 290.000\n",
      "Episode 82/3000\n",
      "Episode 82 finished with total reward: 181.000\n",
      "Episode 83/3000\n",
      "Episode 83 finished with total reward: -38.000\n",
      "Episode 84/3000\n",
      "Episode 84 finished with total reward: 407.000\n",
      "Episode 85/3000\n",
      "Episode 85 finished with total reward: 10.000\n",
      "Episode 86/3000\n",
      "Episode 86 finished with total reward: 274.000\n",
      "Episode 87/3000\n",
      "Episode 87 finished with total reward: -23.000\n",
      "Episode 88/3000\n",
      "Episode 88 finished with total reward: 157.000\n",
      "Episode 89/3000\n",
      "Episode 89 finished with total reward: 386.000\n",
      "Episode 90/3000\n",
      "Episode 90 finished with total reward: -139.000\n",
      "Episode 91/3000\n",
      "Episode 91 finished with total reward: 296.000\n",
      "Episode 92/3000\n",
      "Episode 92 finished with total reward: 43.000\n",
      "Episode 93/3000\n",
      "Episode 93 finished with total reward: 232.000\n",
      "Episode 94/3000\n",
      "Episode 94 finished with total reward: 143.000\n",
      "Episode 95/3000\n",
      "Episode 95 finished with total reward: 173.000\n",
      "Episode 96/3000\n",
      "Episode 96 finished with total reward: 156.000\n",
      "Episode 97/3000\n",
      "Episode 97 finished with total reward: 220.000\n",
      "Episode 98/3000\n",
      "Episode 98 finished with total reward: 191.000\n",
      "Episode 99/3000\n",
      "Episode 99 finished with total reward: -224.000\n",
      "Episode 100/3000\n",
      "Episode 100 finished with total reward: 107.000\n",
      "Episode 100/3000, avg reward: 133.410\n",
      "Best model saved with reward: 133.410\n",
      "Episode 101/3000\n",
      "Episode 101 finished with total reward: 384.000\n",
      "Episode 102/3000\n",
      "Episode 102 finished with total reward: 113.000\n",
      "Episode 103/3000\n",
      "Episode 103 finished with total reward: 324.000\n",
      "Episode 104/3000\n",
      "Episode 104 finished with total reward: 129.000\n",
      "Episode 105/3000\n",
      "Episode 105 finished with total reward: 110.000\n",
      "Episode 106/3000\n",
      "Episode 106 finished with total reward: 29.000\n",
      "Episode 107/3000\n",
      "Episode 107 finished with total reward: 83.000\n",
      "Episode 108/3000\n",
      "Episode 108 finished with total reward: 175.000\n",
      "Episode 109/3000\n",
      "Episode 109 finished with total reward: 267.000\n",
      "Episode 110/3000\n",
      "Episode 110 finished with total reward: 343.000\n",
      "Episode 111/3000\n",
      "Episode 111 finished with total reward: 402.000\n",
      "Episode 112/3000\n",
      "Episode 112 finished with total reward: 250.000\n",
      "Episode 113/3000\n",
      "Episode 113 finished with total reward: 18.000\n",
      "Episode 114/3000\n",
      "Episode 114 finished with total reward: -25.000\n",
      "Episode 115/3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 20\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     next_obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     22\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstore_reward(reward)\n",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m, in \u001b[0;36mPPOAgent.act\u001b[0;34m(self, state, action_mask)\u001b[0m\n\u001b[1;32m     51\u001b[0m     action_logits \u001b[38;5;241m=\u001b[39m action_logits \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e9\u001b[39m\n\u001b[1;32m     53\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39maction_logits)\n\u001b[0;32m---> 54\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(state)\n",
      "File \u001b[0;32m~/miniconda3/envs/mjx_env/lib/python3.8/site-packages/torch/distributions/categorical.py:133\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    131\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    132\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 133\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 训练超参数\n",
    "num_episodes = 3000\n",
    "log_interval = 100  # 每 N 个 episode 记录一次\n",
    "rolling_rewards = []\n",
    "\n",
    "all_rewards = []\n",
    "all_actor_loss = []\n",
    "all_value_loss = []\n",
    "best_reward = -float(\"inf\")\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    print(f\"Episode {episode}/{num_episodes}\")\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(obs, info[\"action_mask\"])\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        agent.store_reward(reward)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    \n",
    "    stats = agent.update()\n",
    "    all_rewards.append(total_reward)\n",
    "    all_actor_loss.append(stats['actor_loss'])\n",
    "    all_value_loss.append(stats['value_loss'])\n",
    "    print(f\"Episode {episode} finished with total reward: {total_reward:.3f}\")\n",
    "\n",
    "    # 打印日志\n",
    "    if episode % log_interval == 0:\n",
    "        avg_reward = np.mean(all_rewards[-log_interval:])\n",
    "        rolling_rewards.append(avg_reward)\n",
    "        print(f\"Episode {episode}/{num_episodes}, avg reward: {avg_reward:.3f}\")\n",
    "        # 更新最佳模型\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            torch.save(agent.model.state_dict(), \"logs/ppo_cr/ppo_cr_lr_1e-4.pt\")\n",
    "            print(f\"Best model saved with reward: {best_reward:.3f}\")\n",
    "\n",
    "        log_data = {\n",
    "            'episode': episode,\n",
    "            'avg_reward': avg_reward,\n",
    "            'actor_loss': stats['actor_loss'],\n",
    "            'value_loss': stats['value_loss'],\n",
    "            'entropy': stats['entropy'],\n",
    "            'total_loss': stats['total_loss']\n",
    "        }\n",
    "        with open('logs/ppo_cr/training_log.json', 'a') as f:\n",
    "            json.dump(log_data, f)\n",
    "            f.write('\\n')\n",
    "# 绘制训练曲线\n",
    "plot_rewards(rolling_rewards, path=f\"logs/ppo_cr/reward_curve_{episode}.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c800c",
   "metadata": {},
   "source": [
    "lr = 0.01, time = 351m 46.9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e9f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-120.15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
