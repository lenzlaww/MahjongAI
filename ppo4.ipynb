{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import mjx\n",
    "from mjx.agents import RandomAgent, ShantenAgent\n",
    "from ppo_agent import PPOAgent, GymEnv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef02922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards, path=\"logs/reward_curve.png\"):\n",
    "    plt.figure()\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Training Reward Curve\")\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf725ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def train_curriculum_agent(\n",
    "    info_type = \"default\", opponents = list(RandomAgent() for _ in range(3)),\n",
    "    num_episodes=1000, log_interval = 100,\n",
    "    pretrained_model=None,\n",
    "    stage = 1,\n",
    "    lr = 1e-4,\n",
    "    all_logs = {\n",
    "        \"all_rewards\": [],\n",
    "        \"all_actor_loss\": [],\n",
    "        \"all_value_loss\": [],\n",
    "    },\n",
    "    \n",
    "):\n",
    "    env = GymEnv(opponent_agents=opponents, info_type=info_type, discard_model=False)\n",
    "    obs, info = env.reset()\n",
    "    obs_shape = obs.flatten().shape[0]\n",
    "    action_dim = len(info[\"action_mask\"])\n",
    "    agent = PPOAgent(\n",
    "        input_dim=obs_shape,\n",
    "        hidden_dim=128,\n",
    "        output_dim=action_dim,\n",
    "        pretrained_model=pretrained_model, # If have a pretrained model, load it\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    rolling_rewards = []\n",
    "    best_reward = -np.inf\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(obs, info[\"action_mask\"])\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "            agent.store_reward(reward)\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "\n",
    "        stats = agent.update(next_obs, done)\n",
    "        all_logs[\"all_actor_loss\"].append(stats[\"actor_loss\"])\n",
    "        all_logs[\"all_value_loss\"].append(stats[\"value_loss\"])\n",
    "        all_logs[\"all_rewards\"].append(total_reward)\n",
    "\n",
    "        if episode % log_interval == 0:\n",
    "            avg_reward = np.mean(all_logs[\"all_rewards\"][-log_interval:])\n",
    "            rolling_rewards.append(avg_reward)\n",
    "            print(f\"Episode {episode}/{num_episodes}, avg reward: {avg_reward:.3f}\")\n",
    "            # Update the best model if the average reward is higher than the previous best\n",
    "            if avg_reward > best_reward:\n",
    "                best_reward = avg_reward\n",
    "                torch.save(agent.model.state_dict(), f\"logs/ppo4/best_model_ppo4_stage_{stage}.pt\")\n",
    "                print(f\"Best model saved with reward: {best_reward:.3f}\")\n",
    "            \n",
    "            log_data = {\n",
    "                \"episode\": episode,\n",
    "                \"avg_reward\": avg_reward,\n",
    "                \"actor_loss\": stats[\"actor_loss\"],\n",
    "                \"value_loss\": stats[\"value_loss\"],\n",
    "                \"entropy\": stats[\"entropy\"],\n",
    "                \"total_loss\": stats[\"total_loss\"],\n",
    "            }\n",
    "\n",
    "            with open(f\"logs/ppo4/stage_{stage}_logs.json\", \"a\") as f:\n",
    "                f.write(json.dumps(log_data) + \"\\n\")\n",
    "            print(f\"Episode {episode} logs saved.\")\n",
    "\n",
    "    plot_rewards(rolling_rewards, path=f\"logs/ppo4/stage_{stage}_reward_curve.png\")\n",
    "    return agent, all_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03117e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Stage 1: Perfect Information with Random Agent Opponents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenzlaww/document/SBU/CSE537/finalProject/MahjongAI/discard_tile/discard.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=DEVICE)['model_state'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/400, avg reward: 282.610\n",
      "Best model saved with reward: 282.610\n",
      "Episode 100 logs saved.\n",
      "Episode 200/400, avg reward: 338.910\n",
      "Best model saved with reward: 338.910\n",
      "Episode 200 logs saved.\n",
      "Episode 300/400, avg reward: 377.290\n",
      "Best model saved with reward: 377.290\n",
      "Episode 300 logs saved.\n",
      "Episode 400/400, avg reward: 376.370\n",
      "Episode 400 logs saved.\n",
      "Training Stage 2: Perfect Information with Shanten Agent Opponents\n",
      "Loaded pretrained model from logs/ppo4/best_model_ppo4_stage_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_880117/4063288015.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_state_dict = torch.load(pretrained_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/600, avg reward: 381.670\n",
      "Best model saved with reward: 381.670\n",
      "Episode 100 logs saved.\n",
      "Episode 200/600, avg reward: 434.180\n",
      "Best model saved with reward: 434.180\n",
      "Episode 200 logs saved.\n",
      "Episode 300/600, avg reward: 422.700\n",
      "Episode 300 logs saved.\n",
      "Episode 400/600, avg reward: 515.050\n",
      "Best model saved with reward: 515.050\n",
      "Episode 400 logs saved.\n",
      "Episode 500/600, avg reward: 534.830\n",
      "Best model saved with reward: 534.830\n",
      "Episode 500 logs saved.\n",
      "Episode 600/600, avg reward: 506.340\n",
      "Episode 600 logs saved.\n",
      "Training Stage 3: Imperfect Information with Shanten Agent Opponents\n",
      "Loaded pretrained model from logs/ppo4/best_model_ppo4_stage_2.pt\n",
      "Episode 100/900, avg reward: 449.490\n",
      "Best model saved with reward: 449.490\n",
      "Episode 100 logs saved.\n",
      "Episode 200/900, avg reward: 523.120\n",
      "Best model saved with reward: 523.120\n",
      "Episode 200 logs saved.\n",
      "Episode 300/900, avg reward: 523.350\n",
      "Best model saved with reward: 523.350\n",
      "Episode 300 logs saved.\n",
      "Episode 400/900, avg reward: 564.460\n",
      "Best model saved with reward: 564.460\n",
      "Episode 400 logs saved.\n",
      "Episode 500/900, avg reward: 534.580\n",
      "Episode 500 logs saved.\n",
      "Episode 600/900, avg reward: 480.990\n",
      "Episode 600 logs saved.\n",
      "Episode 700/900, avg reward: 435.760\n",
      "Episode 700 logs saved.\n",
      "Episode 800/900, avg reward: 506.460\n",
      "Episode 800 logs saved.\n",
      "Episode 900/900, avg reward: 560.610\n",
      "Episode 900 logs saved.\n",
      "Training Stage 4: Self-Play\n",
      "Loaded pretrained model from logs/ppo4/best_model_ppo4_stage_3.pt\n",
      "Episode 100/1100, avg reward: 529.670\n",
      "Best model saved with reward: 529.670\n",
      "Episode 100 logs saved.\n",
      "Episode 200/1100, avg reward: 528.970\n",
      "Episode 200 logs saved.\n",
      "Episode 300/1100, avg reward: 411.070\n",
      "Episode 300 logs saved.\n",
      "Episode 400/1100, avg reward: 422.100\n",
      "Episode 400 logs saved.\n",
      "Episode 500/1100, avg reward: 431.340\n",
      "Episode 500 logs saved.\n",
      "Episode 600/1100, avg reward: 447.920\n",
      "Episode 600 logs saved.\n",
      "Episode 700/1100, avg reward: 443.370\n",
      "Episode 700 logs saved.\n",
      "Episode 800/1100, avg reward: 406.810\n",
      "Episode 800 logs saved.\n",
      "Episode 900/1100, avg reward: 393.120\n",
      "Episode 900 logs saved.\n",
      "Episode 1000/1100, avg reward: 402.430\n",
      "Episode 1000 logs saved.\n",
      "Episode 1100/1100, avg reward: 423.270\n",
      "Episode 1100 logs saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Stage 1: Perfect Information with Random Agent Opponents\")\n",
    "\n",
    "stage_1_agent, all_logs = train_curriculum_agent(\n",
    "    info_type=\"perfect\",\n",
    "    opponents=[RandomAgent() for _ in range(3)],\n",
    "    num_episodes=400,\n",
    "    log_interval=100,\n",
    "    pretrained_model=None,\n",
    "    stage=1,\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"Training Stage 2: Perfect Information with Shanten Agent Opponents\")\n",
    "stage_2_agent, all_logs = train_curriculum_agent(\n",
    "    info_type=\"perfect\",\n",
    "    opponents=[ShantenAgent() for _ in range(3)],\n",
    "    num_episodes=600,\n",
    "    log_interval=100,\n",
    "    pretrained_model=\"logs/ppo4/best_model_ppo4_stage_1.pt\",\n",
    "    stage=2,\n",
    "    all_logs=all_logs,\n",
    "    lr = 5e-4,\n",
    ")\n",
    "\n",
    "print(\"Training Stage 3: Imperfect Information with Shanten Agent Opponents\")\n",
    "stage_3_agent, all_logs = train_curriculum_agent(\n",
    "    info_type=\"default\",\n",
    "    opponents=[ShantenAgent() for _ in range(3)],\n",
    "    num_episodes=900,\n",
    "    log_interval=100,\n",
    "    pretrained_model=\"logs/ppo4/best_model_ppo4_stage_2.pt\",\n",
    "    stage=3,\n",
    "    all_logs=all_logs,\n",
    "    lr = 5e-4\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Training Stage 4: Self-Play\")\n",
    "stage_3_agent, all_logs = train_curriculum_agent(\n",
    "    info_type=\"default\",\n",
    "    opponents=[ShantenAgent() for _ in range(3)],\n",
    "    num_episodes=1100,\n",
    "    log_interval=100,\n",
    "    pretrained_model=\"logs/ppo4/best_model_ppo4_stage_3.pt\",\n",
    "    stage=4,\n",
    "    all_logs=all_logs,\n",
    "    lr=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162c521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards_with_std(rewards, window=10, path=\"reward_curve_with_std.png\"):\n",
    "    rewards = np.array(rewards)\n",
    "    episodes = np.arange(len(rewards))\n",
    "\n",
    "    # Compute rolling mean and std\n",
    "    rolling_mean = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    rolling_std = np.array([np.std(rewards[max(0, i - window):i + 1]) for i in range(window - 1, len(rewards))])\n",
    "\n",
    "    # Align x-axis for rolling mean\n",
    "    rolling_episodes = episodes[window - 1:]\n",
    "\n",
    "    # Ensure that rolling_episodes and rolling_mean are the same length\n",
    "    if len(rolling_episodes) != len(rolling_mean):\n",
    "        print(\"Warning: Rolling episodes and mean length mismatch.\")\n",
    "        return\n",
    "\n",
    "    # Plot the reward curve with shaded area for std\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rolling_episodes, rolling_mean, label='Rolling Average Reward', color='blue')\n",
    "    plt.fill_between(rolling_episodes,\n",
    "                     rolling_mean - rolling_std,\n",
    "                     rolling_mean + rolling_std,\n",
    "                     color='blue', alpha=0.3, label='±1 Std Dev')\n",
    "\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.title(\"Training Reward Curve with Standard Deviation\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "# Example usage\n",
    "plot_rewards_with_std(all_logs[\"all_rewards\"], window=100, path=\"logs/ppo4/whole_reward_curve_with_std.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d782250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "for i in range(1, 5):\n",
    "    with open(f\"logs/ppo4/stage_{i}_logs.json\", \"r\") as f:\n",
    "        for line in f:\n",
    "            logs.append(json.loads(line))\n",
    "if logs:\n",
    "    all_logs = {\n",
    "        \"all_rewards\": [],\n",
    "    }\n",
    "    for log in logs:\n",
    "        all_logs[\"all_rewards\"].append(log[\"avg_reward\"])\n",
    "        \n",
    "    plot_rewards(all_logs[\"all_rewards\"], path=\"logs/ppo4/whole_reward_curve.png\")\n",
    "else:\n",
    "    print(\"No logs found to process.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
