{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831bf3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import mjx\n",
    "from mjx.agents import RandomAgent, ShantenAgent\n",
    "from ppo_agent import PPOAgent, GymEnv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment: Play against Shanten agent\n",
    "opponents = [ShantenAgent() for _ in range(3)]  \n",
    "env = GymEnv(opponent_agents=opponents)\n",
    "\n",
    "# obtain the observation and action mask shape\n",
    "obs, info = env.reset()\n",
    "obs_shape = obs.flatten().shape[0]\n",
    "action_dim = len(info[\"action_mask\"])  # action number\n",
    "\n",
    "# Initialize PPO agent\n",
    "agent = PPOAgent(\n",
    "    input_dim=obs_shape,\n",
    "    hidden_dim=128,\n",
    "    output_dim=action_dim,\n",
    "    lr = 1e-4,              # small learning rate\n",
    "    entropy_coef=0.001      # small entropy coefficient: small curiousity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ef02922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rewards(rewards, path=\"logs/reward_curve.png\"):\n",
    "    plt.figure()\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Training Reward Curve\")\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ccfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/10000, avg reward: -128.250\n",
      "Best model saved with reward: -128.250\n",
      "Episode 200/10000, avg reward: -126.900\n",
      "Best model saved with reward: -126.900\n",
      "Episode 300/10000, avg reward: -120.150\n",
      "Best model saved with reward: -120.150\n",
      "Episode 400/10000, avg reward: -124.200\n",
      "Episode 500/10000, avg reward: -128.250\n",
      "Episode 600/10000, avg reward: -124.200\n",
      "Episode 700/10000, avg reward: -126.900\n",
      "Episode 800/10000, avg reward: -126.450\n",
      "Episode 900/10000, avg reward: -122.850\n",
      "Episode 1000/10000, avg reward: -121.500\n",
      "Episode 1100/10000, avg reward: -126.900\n",
      "Episode 1200/10000, avg reward: -120.150\n",
      "Episode 1300/10000, avg reward: -132.300\n",
      "Episode 1400/10000, avg reward: -119.700\n",
      "Best model saved with reward: -119.700\n",
      "Episode 1500/10000, avg reward: -119.700\n",
      "Episode 1600/10000, avg reward: -121.500\n",
      "Episode 1700/10000, avg reward: -124.200\n",
      "Episode 1800/10000, avg reward: -121.500\n",
      "Episode 1900/10000, avg reward: -122.850\n",
      "Episode 2000/10000, avg reward: -124.650\n",
      "Episode 2100/10000, avg reward: -121.050\n",
      "Episode 2200/10000, avg reward: -122.850\n",
      "Episode 2300/10000, avg reward: -117.000\n",
      "Best model saved with reward: -117.000\n",
      "Episode 2400/10000, avg reward: -123.750\n",
      "Episode 2500/10000, avg reward: -124.650\n",
      "Episode 2600/10000, avg reward: -115.200\n",
      "Best model saved with reward: -115.200\n",
      "Episode 2700/10000, avg reward: -123.300\n",
      "Episode 2800/10000, avg reward: -125.100\n",
      "Episode 2900/10000, avg reward: -125.100\n",
      "Episode 3000/10000, avg reward: -126.450\n",
      "Episode 3100/10000, avg reward: -122.400\n",
      "Episode 3200/10000, avg reward: -116.550\n",
      "Episode 3300/10000, avg reward: -122.850\n",
      "Episode 3400/10000, avg reward: -118.800\n",
      "Episode 3500/10000, avg reward: -118.350\n",
      "Episode 3600/10000, avg reward: -129.600\n",
      "Episode 3700/10000, avg reward: -119.250\n",
      "Episode 3800/10000, avg reward: -122.400\n",
      "Episode 3900/10000, avg reward: -116.550\n",
      "Episode 4000/10000, avg reward: -119.700\n",
      "Episode 4100/10000, avg reward: -120.150\n",
      "Episode 4200/10000, avg reward: -117.450\n",
      "Episode 4300/10000, avg reward: -113.850\n",
      "Best model saved with reward: -113.850\n",
      "Episode 4400/10000, avg reward: -119.250\n",
      "Episode 4500/10000, avg reward: -129.150\n",
      "Episode 4600/10000, avg reward: -127.350\n",
      "Episode 4700/10000, avg reward: -118.350\n",
      "Episode 4800/10000, avg reward: -124.200\n",
      "Episode 4900/10000, avg reward: -121.050\n",
      "Episode 5000/10000, avg reward: -122.850\n",
      "Episode 5100/10000, avg reward: -128.250\n",
      "Episode 5200/10000, avg reward: -124.200\n",
      "Episode 5300/10000, avg reward: -124.200\n",
      "Episode 5400/10000, avg reward: -129.600\n",
      "Episode 5500/10000, avg reward: -125.100\n",
      "Episode 5600/10000, avg reward: -122.400\n",
      "Episode 5700/10000, avg reward: -128.250\n",
      "Episode 5800/10000, avg reward: -126.450\n",
      "Episode 5900/10000, avg reward: -126.900\n",
      "Episode 6000/10000, avg reward: -126.900\n",
      "Episode 6100/10000, avg reward: -124.200\n",
      "Episode 6200/10000, avg reward: -123.750\n",
      "Episode 6300/10000, avg reward: -132.300\n",
      "Episode 6400/10000, avg reward: -129.600\n",
      "Episode 6500/10000, avg reward: -126.450\n",
      "Episode 6600/10000, avg reward: -128.250\n",
      "Episode 6700/10000, avg reward: -128.250\n",
      "Episode 6800/10000, avg reward: -121.950\n",
      "Episode 6900/10000, avg reward: -124.200\n",
      "Episode 7000/10000, avg reward: -126.900\n",
      "Episode 7100/10000, avg reward: -126.900\n",
      "Episode 7200/10000, avg reward: -126.900\n",
      "Episode 7300/10000, avg reward: -125.100\n",
      "Episode 7400/10000, avg reward: -129.150\n",
      "Episode 7500/10000, avg reward: -129.600\n",
      "Episode 7600/10000, avg reward: -124.200\n",
      "Episode 7700/10000, avg reward: -123.750\n",
      "Episode 7800/10000, avg reward: -125.550\n",
      "Episode 7900/10000, avg reward: -132.300\n",
      "Episode 8000/10000, avg reward: -128.250\n",
      "Episode 8100/10000, avg reward: -122.850\n",
      "Episode 8200/10000, avg reward: -128.250\n",
      "Episode 8300/10000, avg reward: -121.500\n",
      "Episode 8400/10000, avg reward: -125.100\n",
      "Episode 8500/10000, avg reward: -126.900\n",
      "Episode 8600/10000, avg reward: -129.600\n",
      "Episode 8700/10000, avg reward: -124.200\n",
      "Episode 8800/10000, avg reward: -130.950\n",
      "Episode 8900/10000, avg reward: -125.100\n",
      "Episode 9000/10000, avg reward: -127.800\n",
      "Episode 9100/10000, avg reward: -125.100\n",
      "Episode 9200/10000, avg reward: -115.650\n",
      "Episode 9300/10000, avg reward: -128.250\n",
      "Episode 9400/10000, avg reward: -122.850\n",
      "Episode 9500/10000, avg reward: -125.100\n",
      "Episode 9600/10000, avg reward: -122.400\n",
      "Episode 9700/10000, avg reward: -117.450\n",
      "Episode 9800/10000, avg reward: -128.250\n",
      "Episode 9900/10000, avg reward: -125.550\n",
      "Episode 10000/10000, avg reward: -120.150\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Training hyperparameters\n",
    "num_episodes = 3000\n",
    "log_interval = 100  # record every 100 episodes\n",
    "rolling_rewards = []\n",
    "\n",
    "all_rewards = []\n",
    "all_actor_loss = []\n",
    "all_value_loss = []\n",
    "best_reward = -float(\"inf\")\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    obs, info = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(obs, info[\"action_mask\"])\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        agent.store_reward(reward)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "    \n",
    "    stats = agent.update()\n",
    "    all_rewards.append(total_reward)\n",
    "    all_actor_loss.append(stats['actor_loss'])\n",
    "    all_value_loss.append(stats['value_loss'])\n",
    "\n",
    "    # logs\n",
    "    if episode % log_interval == 0:\n",
    "        avg_reward = np.mean(all_rewards[-log_interval:])\n",
    "        rolling_rewards.append(avg_reward)\n",
    "        print(f\"Episode {episode}/{num_episodes}, avg reward: {avg_reward:.3f}\")\n",
    "        # update the best model\n",
    "        if avg_reward > best_reward:\n",
    "            best_reward = avg_reward\n",
    "            torch.save(agent.model.state_dict(), \"logs/ppo/ppo_shanten_model_1e-4_10000.pt\")\n",
    "            print(f\"Best model saved with reward: {best_reward:.3f}\")\n",
    "\n",
    "        log_data = {\n",
    "            'episode': episode,\n",
    "            'avg_reward': avg_reward,\n",
    "            'actor_loss': stats['actor_loss'],\n",
    "            'value_loss': stats['value_loss'],\n",
    "            'entropy': stats['entropy'],\n",
    "            'total_loss': stats['total_loss']\n",
    "        }\n",
    "        with open('logs/ppo/ppo_shanten_training_log_1e-4_10000.json', 'a') as f:\n",
    "            json.dump(log_data, f)\n",
    "            f.write('\\n')\n",
    "# plot the reward curve\n",
    "plot_rewards(rolling_rewards, path=f\"logs/ppo/ppo_shanten_reward_curve_{episode}_1e-4_10000.png\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mjx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
